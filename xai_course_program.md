# TÍTULO: Inteligencia Artificial Explicable 

**AÑO:** 2025 **CUATRIMESTRE:** 1° **N° DE CRÉDITOS:** 3 **VIGENCIA:** 3 años  
**CARGA HORARIA:** 60 horas de teoría y 60 horas de práctica  
**CARRERA/S:** Doctorado en Ciencias de la Computación, Doctorado en Matemática, Doctorado en Física, Doctorado en Astronomía

## FUNDAMENTOS
La inteligencia artificial explicable (XAI) ha emergido como un campo crítico debido a la creciente adopción de sistemas de machine learning en dominios de alto riesgo como healthcare, finanzas y justicia. Este programa está diseñado para proporcionar una comprensión profunda de los métodos y procesos que permiten a los usuarios humanos comprender y confiar en los resultados generados por algoritmos de IA. El curso abarca desde fundamentos conceptuales hasta desarrollos vanguardistas como interpretabilidad mecanística y modelos multimodales, preparando a los estudiantes para los desafíos emergentes del campo.

## OBJETIVOS
Que el alumno comprenda los principios fundamentales de la interpretabilidad y explicabilidad en sistemas de IA, desarrolle competencias en la aplicación de métodos tradicionales y avanzados de XAI, y adquiera la capacidad de evaluar críticamente diferentes enfoques de explicación. Se busca proporcionar herramientas teórico-prácticas para el diseño de sistemas explicables, el análisis de circuitos neurales, y la implementación de frameworks de evaluación robustos. Además, se pretende que el alumno desarrolle criterio para seleccionar métodos apropiados según el contexto de aplicación y los stakeholders involucrados.

## PROGRAMA

### Unidad I: Fundamentos y Factores Humanos

Introducción a la inteligencia artificial explicable: definiciones, objetivos y taxonomías. Contextualización histórica desde modelos interpretables hasta deep learning. Test de Turing para explicaciones. Factores humanos en explicabilidad: necesidades de diferentes usuarios finales (médicos, ingenieros ML, reguladores). Estudios empíricos sobre comprensión humana de explicaciones. Sesgos cognitivos y limitaciones en la interpretación de explicaciones. Marcos conceptuales para diseño centrado en humanos. Diferencias entre interpretabilidad, explicabilidad y transparencia.

### Unidad II: Métodos de Interpretabilidad y Explicabilidad  

Modelos inherentemente interpretables: árboles de decisión, regresión lineal, reglas bayesianas. Ventajas y limitaciones de la interpretabilidad by-design. Explicaciones post-hoc: atribuciones de características (LIME, SHAP, Integrated Gradients). Fundamentos matemáticos de métodos de atribución. Pitfalls y desafíos: robustez, estabilidad y sanity checks. Explicaciones contrafactuales y recourse algorítmico. Generación de contrafactuales factibles y válidos. Evaluación crítica de métodos de atribución tradicionales.

### Unidad III: Técnicas Avanzadas y Evaluación
**Semana 7-10**

Explicaciones basadas en atención y conceptos: análisis de attention maps, concept activation vectors (CAVs). Atribución de datos e influencia: influence functions, data shapley. Explicaciones interactivas y interfaces de usuario. Teoría de la explicabilidad: axiomas de Shapley, propiedades deseables. Interpretación de modelos generativos: GANs, VAEs, diffusion models. Conexiones con robustez, privacidad, fairness y machine unlearning. Frameworks de evaluación integral y métricas de calidad.

### Unidad IV: Interpretabilidad de Modelos Grandes
**Semana 11-13**

Interpretabilidad mecanística: reverse engineering de redes neuronales. Análisis de transformers compilados y circuitos computacionales. Comprensión y razonamiento en Large Language Models: emergent abilities, in-context learning, chain-of-thought reasoning. Interpretación de otros modelos grandes: vision transformers, modelos multimodales. Técnicas de análisis causal en arquitecturas transformer. Probing tasks y representational analysis. Scaling laws para interpretabilidad.

### Unidad V: Fronteras Emergentes de XAI  

Automated Circuit Discovery (ACDC): identificación automática de circuitos neurales. Sparse Autoencoders (SAEs): descomposición de neuronas polisemánticas. Path patching y activation patching para rastreo de información. Linear Representation Hypothesis y direcciones interpretables. XAI multimodal: modelos vision-language, cross-modal attention analysis. Framework MAIA para interpretabilidad automatizada. Explicación de alucinaciones en modelos multimodales. F-Fidelity y M4 benchmark para evaluación avanzada. Tendencias futuras y desafíos abiertos.

## PRÁCTICAS


## BIBLIOGRAFÍA

**Textos fundamentales:**
- Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning.
- Lipton, Z. C. (2018). The mythos of model interpretability. Communications of the ACM.
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier.

**Desarrollos recientes:**
- Nanda, N., et al. (2023). Progress measures for grokking via mechanistic interpretability.
- Conmy, A., et al. (2023). Towards automated circuit discovery for mechanistic interpretability.
- Tsai, C., et al. (2023). MAIA: Multimodal automated interpretability agent.

**Evaluación y teoría:**
- Krishna, S., et al. (2023). Post hoc explanations may be ineffective for detecting unknown spurious correlation.
- Covert, I., et al. (2021). Explaining by removing: A unified framework for model explanation.

## MODALIDAD DE EVALUACIÓN
Para regularizar: aprobar los cuatro trabajos prácticos mediante implementación práctica e informe técnico.

Para aprobar: examen oral final con presentación de proyecto integrador relacionado con línea de investigación del estudiante, demostrando dominio teórico-práctico de técnicas de XAI.

## Cronograma aproximado

| Unidad | Título | Semanas |
|--------|--------|---------|
| I | Fundamentos y Factores Humanos | 1-2 |
| II | Métodos de Interpretabilidad y Explicabilidad | 3-6 |
| III | Técnicas Avanzadas y Evaluación | 7-10 |
| IV | Interpretabilidad de Modelos Grandes | 11-13 |
| V | Fronteras Emergentes de XAI | 14-15 |

## REQUERIMIENTOS PARA EL CURSADO
Conocimientos sólidos de machine learning, álgebra lineal, cálculo multivariable y programación científica (Python). Familiaridad con frameworks de deep learning (PyTorch/TensorFlow). Experiencia previa con redes neuronales y modelos de lenguaje recomendada pero no indispensable.
